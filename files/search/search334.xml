Touchpad A:@0.176420:0.958597:0.262602:0.958597:0.262602:0.941370:0.176420:0.941370:0.008484:0.009106:0.009106:0.008189:0.009106:0.009106:0.009106:0.009123:0.003931:0.010924
rtificial Intelligence (Ver. 3.0)-XI:@0.262897:0.958597:0.489524:0.958597:0.489524:0.941370:0.262897:0.941370:0.005454:0.004553:0.003636:0.004095:0.004095:0.008189:0.003636:0.009106:0.003636:0.004553:0.004553:0.009106:0.004553:0.009106:0.003636:0.003636:0.003636:0.009106:0.009106:0.009106:0.008189:0.009106:0.004553:0.005454:0.010041:0.009106:0.004553:0.004553:0.004553:0.009106:0.004553:0.009106:0.005454:0.005749:0.011499:0.004553
332:@0.113927:0.958672:0.140411:0.958672:0.140411:0.942265:0.113927:0.942265:0.008828:0.008828:0.008828
Advantage of Linear Regression :@0.066874:0.063162:0.365632:0.063162:0.365632:0.046403:0.066874:0.046403:0.013817:0.012166:0.010652:0.010574:0.011891:0.007645:0.010574:0.012166:0.010633:0.005424:0.012008:0.007527:0.005424:0.010043:0.005582:0.011891:0.010633:0.010574:0.007822:0.005424:0.012834:0.010633:0.012166:0.007822:0.010633:0.008648:0.008648:0.005582:0.012008:0.011891:0.005424
Some advantages of linear regression are as follows::@0.066874:0.086116:0.450711:0.086116:0.450711:0.069709:0.066874:0.069709:0.008861:0.009761:0.014265:0.008729:0.004651:0.008500:0.009811:0.007712:0.008500:0.009434:0.005716:0.008500:0.009811:0.008730:0.007107:0.004651:0.009465:0.005290:0.004651:0.004127:0.004127:0.009434:0.008730:0.008500:0.005863:0.004651:0.005641:0.008730:0.009811:0.005641:0.008730:0.007108:0.007108:0.004127:0.009761:0.009433:0.004651:0.008500:0.005642:0.008729:0.004651:0.008500:0.007108:0.004651:0.005290:0.009761:0.004127:0.004127:0.009761:0.012005:0.007108:0.003554
●  Linear regression is a simple technique and easy to implement.:@0.066874:0.108624:0.556376:0.108624:0.556376:0.092217:0.066874:0.092217:0.010056:0.004488:0.013312:0.007878:0.004127:0.009434:0.008730:0.008500:0.005863:0.004651:0.005641:0.008730:0.009811:0.005641:0.008730:0.007108:0.007108:0.004127:0.009761:0.009433:0.004651:0.004127:0.007108:0.004651:0.008500:0.004651:0.007108:0.004127:0.014265:0.009794:0.004127:0.008729:0.004651:0.005587:0.008730:0.007730:0.009434:0.009434:0.004127:0.009811:0.009434:0.008729:0.004651:0.008500:0.009434:0.009810:0.004651:0.008730:0.008500:0.007108:0.008090:0.004651:0.005587:0.009761:0.004651:0.004127:0.014265:0.009794:0.004127:0.008730:0.014265:0.008730:0.009434:0.005716:0.003554
●  Efficient to train the machine on this model.:@0.066874:0.131133:0.416936:0.131133:0.416936:0.114726:0.066874:0.114726:0.010056:0.004488:0.013312:0.008451:0.005290:0.004627:0.004627:0.007730:0.004127:0.008730:0.009434:0.005715:0.004651:0.005587:0.009761:0.004651:0.005716:0.005863:0.008500:0.004127:0.009433:0.004651:0.005716:0.009434:0.008729:0.004651:0.014265:0.008500:0.007730:0.009434:0.004127:0.009434:0.008729:0.004651:0.009761:0.009434:0.004651:0.005716:0.009434:0.004127:0.007108:0.004651:0.014265:0.009761:0.009811:0.008730:0.004127:0.003554
Disadvantages of Linear Regression:@0.066874:0.166663:0.394326:0.166663:0.394326:0.149904:0.066874:0.149904:0.014485:0.005582:0.008648:0.010574:0.012166:0.010652:0.010574:0.011891:0.007645:0.010574:0.012166:0.010633:0.008648:0.005424:0.012008:0.007527:0.005424:0.010043:0.005582:0.011891:0.010633:0.010574:0.007822:0.005424:0.012834:0.010633:0.012166:0.007822:0.010633:0.008648:0.008648:0.005582:0.012008:0.011891
Some disadvantages of linear regression are as follows::@0.066874:0.189616:0.471759:0.189616:0.471759:0.173209:0.066874:0.173209:0.008861:0.009761:0.014265:0.008729:0.004651:0.009811:0.004127:0.007108:0.008500:0.009811:0.007712:0.008500:0.009434:0.005716:0.008500:0.009811:0.008730:0.007107:0.004651:0.009465:0.005290:0.004651:0.004127:0.004127:0.009434:0.008730:0.008500:0.005863:0.004651:0.005642:0.008730:0.009811:0.005642:0.008730:0.007108:0.007108:0.004127:0.009761:0.009433:0.004651:0.008500:0.005642:0.008729:0.004651:0.008500:0.007108:0.004651:0.005290:0.009761:0.004127:0.004127:0.009761:0.012005:0.007108:0.003554
●  Regression analysis is sensitive to outliers as these can have a great impact on the analysis.:@0.066874:0.212125:0.761998:0.212125:0.761998:0.195718:0.066874:0.195718:0.010056:0.004488:0.013312:0.009504:0.008730:0.009811:0.005641:0.008730:0.007108:0.007108:0.004127:0.009761:0.009433:0.004651:0.008500:0.009434:0.008500:0.004127:0.008091:0.007108:0.004127:0.007107:0.004651:0.004127:0.007108:0.004651:0.007108:0.008730:0.009434:0.007108:0.004127:0.005716:0.004127:0.007914:0.008729:0.004651:0.005587:0.009761:0.004651:0.009761:0.009434:0.005716:0.004127:0.004127:0.008730:0.005970:0.007107:0.004651:0.008500:0.007108:0.004651:0.005716:0.009434:0.008730:0.007108:0.008729:0.004651:0.007730:0.008500:0.009434:0.004651:0.009434:0.008500:0.007912:0.008729:0.004651:0.008500:0.004651:0.009811:0.005642:0.008730:0.008500:0.005715:0.004651:0.004127:0.014265:0.009576:0.008500:0.007730:0.005715:0.004651:0.009761:0.009434:0.004651:0.005716:0.009434:0.008729:0.004651:0.008500:0.009434:0.008500:0.004127:0.008091:0.007108:0.004127:0.007108:0.003554
●   It is quite prone to overfitting. (Overfitting means that the training of the model on data is just too good and the :@0.066874:0.234634:0.926334:0.234634:0.926334:0.218227:0.066874:0.218227:0.010056:0.004488:0.013315:0.000000:0.004520:0.005716:0.004586:0.004127:0.007108:0.004602:0.009811:0.009434:0.004127:0.005585:0.008730:0.004586:0.009794:0.005634:0.009761:0.009434:0.008730:0.004586:0.005585:0.009761:0.004586:0.009761:0.007911:0.008730:0.005863:0.004627:0.004627:0.005716:0.005716:0.004127:0.009434:0.009811:0.003718:0.004570:0.005110:0.012513:0.007911:0.008730:0.005863:0.004627:0.004627:0.005716:0.005716:0.004127:0.009434:0.009811:0.004570:0.014265:0.008730:0.008500:0.009434:0.007108:0.004602:0.005716:0.009434:0.008500:0.005716:0.004586:0.005716:0.009434:0.008730:0.004586:0.005716:0.005863:0.008500:0.004127:0.009434:0.004127:0.009434:0.009811:0.004586:0.009467:0.005290:0.004586:0.005716:0.009434:0.008730:0.004586:0.014265:0.009761:0.009811:0.008730:0.004127:0.004602:0.009761:0.009434:0.004586:0.009811:0.008500:0.005716:0.008500:0.004586:0.004127:0.007108:0.004602:0.004127:0.009434:0.007108:0.005716:0.004602:0.005585:0.009761:0.009761:0.004586:0.009811:0.009761:0.009761:0.009811:0.004586:0.008500:0.009434:0.009811:0.004586:0.005716:0.009434:0.008566:0.004488
test sample size is quite small).:@0.094730:0.253565:0.320895:0.253565:0.320895:0.237158:0.094730:0.237158:0.005587:0.008730:0.007108:0.005716:0.004651:0.007108:0.008500:0.014265:0.009794:0.004127:0.008729:0.004651:0.007108:0.004127:0.007567:0.008729:0.004651:0.004127:0.007108:0.004651:0.009811:0.009434:0.004127:0.005585:0.008729:0.004651:0.007108:0.014265:0.008500:0.004127:0.004127:0.005110:0.003554
1.   State the two types of Regression. :@0.111681:0.320672:0.393319:0.320672:0.393319:0.304265:0.111681:0.304265:0.008992:0.003718:0.004488:0.010662:0.000000:0.008336:0.005716:0.008500:0.005585:0.008730:0.004651:0.005716:0.009434:0.008730:0.004651:0.005716:0.011951:0.009766:0.004651:0.005716:0.008091:0.009794:0.008730:0.007108:0.004651:0.009463:0.005277:0.004651:0.009504:0.008730:0.009811:0.005642:0.008730:0.007108:0.007108:0.004127:0.009761:0.009434:0.003721:0.004488
    :@0.716064:0.320672:0.734505:0.320672:0.734505:0.304265:0.716064:0.304265:0.004651:0.004651:0.004651:0.004488
2.  How many variables are used in linear regression? :@0.111681:0.346759:0.510782:0.346759:0.510782:0.330352:0.111681:0.330352:0.008992:0.003718:0.004488:0.010661:0.011792:0.009761:0.012005:0.004651:0.014265:0.008500:0.009434:0.008090:0.004651:0.007712:0.008500:0.005863:0.004127:0.008500:0.009794:0.004127:0.008730:0.007107:0.004651:0.008500:0.005641:0.008729:0.004651:0.009434:0.007108:0.008730:0.009810:0.004651:0.004127:0.009434:0.004651:0.004127:0.004127:0.009434:0.008730:0.008500:0.005863:0.004651:0.005642:0.008730:0.009811:0.005642:0.008730:0.007108:0.007108:0.004127:0.009761:0.009434:0.007500:0.004488
3.  State the equation of the line of best fit. :@0.111681:0.372845:0.437336:0.372845:0.437336:0.356438:0.111681:0.356438:0.008992:0.003718:0.004488:0.010661:0.008336:0.005716:0.008500:0.005587:0.008729:0.004651:0.005716:0.009434:0.008729:0.004651:0.008730:0.009811:0.009434:0.008500:0.005716:0.004127:0.009761:0.009433:0.004651:0.009465:0.005290:0.004651:0.005716:0.009434:0.008729:0.004651:0.004127:0.004127:0.009434:0.008729:0.004651:0.009465:0.005290:0.004651:0.009794:0.008730:0.007108:0.005715:0.004651:0.004627:0.004627:0.005716:0.003718:0.004488
4.  Why is it called the line of best fit? :@0.111681:0.398931:0.396768:0.398931:0.396768:0.382524:0.111681:0.382524:0.008992:0.003718:0.004488:0.010661:0.015461:0.009434:0.008090:0.004651:0.004127:0.007108:0.004651:0.004127:0.005716:0.004651:0.007730:0.008500:0.004127:0.004127:0.008730:0.009810:0.004651:0.005716:0.009434:0.008729:0.004651:0.004127:0.004127:0.009434:0.008729:0.004651:0.009465:0.005290:0.004651:0.009794:0.008730:0.007108:0.005716:0.004651:0.004627:0.004627:0.005290:0.007501:0.004488
5.  State two applications of regression. :@0.111681:0.425017:0.410213:0.425017:0.410213:0.408610:0.111681:0.408610:0.008992:0.003718:0.004488:0.010661:0.008336:0.005716:0.008500:0.005587:0.008729:0.004651:0.005716:0.011953:0.009761:0.004651:0.008500:0.009794:0.009794:0.004127:0.004127:0.007730:0.008500:0.005716:0.004127:0.009761:0.009434:0.007107:0.004651:0.009465:0.005290:0.004651:0.005642:0.008730:0.009811:0.005642:0.008730:0.007108:0.007108:0.004127:0.009761:0.009434:0.003717:0.004488
    :@0.716064:0.425017:0.734505:0.425017:0.734505:0.408610:0.716064:0.408610:0.004651:0.004651:0.004651:0.004488
 Reboot:@0.182247:0.296726:0.305738:0.296726:0.305738:0.265376:0.182247:0.265376:0.009827:0.022012:0.017852:0.020211:0.019621:0.019621:0.014347
For Advanced Learners:@0.066874:0.490536:0.277170:0.490536:0.277170:0.473776:0.066874:0.473776:0.010220:0.012008:0.007822:0.005424:0.013817:0.012166:0.010652:0.010574:0.011891:0.009434:0.010633:0.012166:0.005424:0.010043:0.010633:0.010574:0.007822:0.011891:0.010633:0.007822:0.008648
Program 1: To demonstrate the use of simple linear regression in Python:@0.080802:0.517067:0.637931:0.517067:0.637931:0.500420:0.080802:0.500420:0.010056:0.006458:0.010007:0.010138:0.006519:0.008811:0.015002:0.004520:0.009417:0.004438:0.004520:0.008133:0.010007:0.004520:0.010138:0.008861:0.015002:0.010007:0.009909:0.007206:0.006371:0.006519:0.008811:0.006322:0.008861:0.004520:0.006371:0.009860:0.008861:0.004520:0.009909:0.007206:0.008861:0.004520:0.009763:0.006273:0.004520:0.007206:0.004651:0.015002:0.010154:0.004651:0.008861:0.004520:0.004651:0.004651:0.009909:0.008861:0.008811:0.006519:0.004520:0.006466:0.008861:0.010138:0.006456:0.008861:0.007206:0.007206:0.004651:0.010007:0.009909:0.004520:0.004651:0.009909:0.004520:0.010056:0.008854:0.006371:0.009860:0.010007:0.009909
import numpy as np:@0.066881:0.545743:0.243765:0.545743:0.243765:0.531464:0.066881:0.531464:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
import matplotlib.pyplot as plt:@0.066881:0.567299:0.371514:0.567299:0.371514:0.553020:0.066881:0.553020:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
# Updated sample data with more values:@0.066881:0.610425:0.440303:0.610425:0.440303:0.596146:0.066881:0.596146:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
data_x = np.array([2, 4, 8, 6, 8, 10, 14, 12, 16, 20]):@0.066881:0.631981:0.597533:0.631981:0.597533:0.617702:0.066881:0.617702:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
data_y = np.array([3, 5, 7, 8, 9, 6, 7, 5, 9, 8]):@0.066881:0.653537:0.548399:0.653537:0.548399:0.639258:0.066881:0.639258:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
# Calculate mean and standard deviation:@0.066881:0.696663:0.450130:0.696663:0.450130:0.682384:0.066881:0.682384:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
mean_x = np.mean(data_x):@0.066881:0.718219:0.302726:0.718219:0.302726:0.703940:0.066881:0.703940:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
mean_y = np.mean(data_y):@0.066881:0.739775:0.302726:0.739775:0.302726:0.725496:0.066881:0.725496:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
std_x = np.std(data_x):@0.066881:0.761332:0.283072:0.761332:0.283072:0.747053:0.066881:0.747053:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
std_y = np.std(data_y):@0.066881:0.782888:0.283072:0.782888:0.283072:0.768609:0.066881:0.768609:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
# Calculate covariance and slope:@0.066881:0.826013:0.381341:0.826013:0.381341:0.811734:0.066881:0.811734:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
cov = np.sum((data_x - mean_x) * (data_y - mean_y)) / (len(data_x) - 1):@0.066881:0.847570:0.764590:0.847570:0.764590:0.833291:0.066881:0.833291:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
slope = cov / (std_x**2):@0.066881:0.869126:0.302726:0.869126:0.302726:0.854847:0.066881:0.854847:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
# Calculate y-intercept (b):@0.066881:0.912251:0.332207:0.912251:0.332207:0.897972:0.066881:0.897972:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827
intercept = mean_y - slope * mean_x:@0.066881:0.933808:0.410822:0.933808:0.410822:0.919529:0.066881:0.919529:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827:0.009827